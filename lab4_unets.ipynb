{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv4PWotd14JN"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/porterjenkins/byu-cs474/blob/master/lab4_unets.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ga7iKpg5UoI"
      },
      "source": [
        "# Cancer Detection and UNet\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsGUIDFW14JP"
      },
      "source": [
        "\n",
        "In this lab, we will learn on a dataset for a real-world task. We will be performing cancer detection: given a photo of a slide of cells, identify the areas of cancerous cells. We will be performing image segmentation: classifying each pixel as cancerous or non-cancerous.\n",
        "\n",
        "You will create a large network, using the U-net architecture described below. The main goal of this lab is for you to understand how to create complex network architectures capable of learning difficult tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnHpsA3-5UoK"
      },
      "source": [
        "---\n",
        "## Grading Criteria\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsYjgjIy14JP"
      },
      "source": [
        "* 20% Completed dataset analysis\n",
        "* 20% Correct training and validation functions\n",
        "* 10% Proper creation, training, and validation of smallest-possible network for this task\n",
        "* 40% Proper creation, training, and validation of the U-net architecture\n",
        "* 10% Reasonable validation results achieved with the U-net architecture\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4Tjg0UQ14JQ"
      },
      "source": [
        "---\n",
        "\n",
        "## Lab Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQOefmcZVgTl",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gzip\n",
        "import tarfile\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import transforms, utils, datasets\n",
        "\n",
        "import shutil\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gskL3Ii5UoM"
      },
      "source": [
        "Set the global variable `device = torch.device(\"cpu\")`. You can change this to `cuda` once you know everything is working, that way you don't run out of your GPU resources on Colab.\n",
        "\n",
        "Remember to use a CPU runtime when not using the GPU. Colab has strict limits on how long you can use a GPU runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUxZzf3E5UoN"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49quyvIJvTq8"
      },
      "source": [
        "WARNING: You may run into an error that says `RuntimeError: CUDA out of memory.`\n",
        "\n",
        "If you get this error immediately when training, then your batch is probably larger than what the GPU is capable of. You can solve this problem by adjusting the image size or the batch size and then restarting the runtime.\n",
        "If you get this error after training for multiple steps then you probably are not freeing up computation graph, i.e. you are storing `loss` instead of `loss.item()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Il_53HLSWPTY"
      },
      "outputs": [],
      "source": [
        "class CancerDataset(Dataset):\n",
        "  def __init__(self, root, download=True, size=512, train=True):\n",
        "    if download and not os.path.exists(os.path.join(root, 'cancer_data')):\n",
        "      datasets.utils.download_url('http://liftothers.org/cancer_data.tar.gz', root, 'cancer_data.tar.gz', None)\n",
        "      self.extract_gzip(os.path.join(root, 'cancer_data.tar.gz'))\n",
        "      self.extract_tar(os.path.join(root, 'cancer_data.tar'))\n",
        "\n",
        "    postfix = 'train' if train else 'test'\n",
        "    root = os.path.join(root, 'cancer_data', 'cancer_data')\n",
        "    self.dataset_folder = datasets.ImageFolder(os.path.join(root, 'inputs_' + postfix) ,transform = transforms.Compose([transforms.Resize(size),transforms.ToTensor()]))\n",
        "    self.label_folder = datasets.ImageFolder(os.path.join(root, 'outputs_' + postfix) ,transform = transforms.Compose([transforms.Resize(size),transforms.ToTensor()]))\n",
        "\n",
        "  @staticmethod\n",
        "  def extract_gzip(gzip_path, remove_finished=False):\n",
        "    print('Extracting {}'.format(gzip_path))\n",
        "    with open(gzip_path.replace('.gz', ''), 'wb') as out_f, gzip.GzipFile(gzip_path) as zip_f:\n",
        "      out_f.write(zip_f.read())\n",
        "    if remove_finished:\n",
        "      os.unlink(gzip_path)\n",
        "\n",
        "  @staticmethod\n",
        "  def extract_tar(tar_path):\n",
        "    print('Untarring {}'.format(tar_path))\n",
        "    z = tarfile.TarFile(tar_path)\n",
        "    z.extractall(tar_path.replace('.tar', ''))\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    img = self.dataset_folder[index]\n",
        "    label = self.label_folder[index]\n",
        "    return img[0],label[0][0]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DOVQOsr5UoO"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 1: Analyzing the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zptcyjuu5UoO"
      },
      "source": [
        "First things first.\n",
        "Let's take a look at the dataset.\n",
        "\n",
        "*Note: This may take a while to download; it may be quicker to upload it to Colab from your machine.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOtZKw4A5UoP"
      },
      "outputs": [],
      "source": [
        "train_dataset = CancerDataset(\"/tmp/Datasets/cancer\", train=True)\n",
        "val_dataset = CancerDataset(\"/tmp/Datasets/cancer\", train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpTt-M2J5UoP"
      },
      "source": [
        "Print out the lengths of your train and validation datasets, so you know how much data you are working with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfuThS5a5UoP"
      },
      "outputs": [],
      "source": [
        "print(len(train_dataset))\n",
        "print(len(val_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpKQHdAK5UoQ"
      },
      "source": [
        "Now you should examine a single data instance from the dataset.\n",
        "We'll use pos_test_000072.png (`val_dataset[172]`) as an example throughout training.\n",
        "Collect the image and the cancer labels from pos_test_000072.png (i.e. `val_x, val_y = val_dataset[172]`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RR-zYwwn5UoQ"
      },
      "outputs": [],
      "source": [
        "val_x, val_y = val_dataset[172]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl47ybmY5UoQ"
      },
      "source": [
        "Let's analyze the `x` data first. Print out the shape and dtype of `val_x` below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3UHZ7Ka5UoQ"
      },
      "outputs": [],
      "source": [
        "print(val_x.shape)\n",
        "print(val_x.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBvCiRLf5UoR"
      },
      "source": [
        "`x` has 3 channels, which likely means that `x` is an RGB image (which it is).\n",
        "Since RGB images can be values between [0, 255] or [0, 1] we should print out the min and max values of x to see which.\n",
        "Print out the min and max values of x below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tb-HJFyV5UoR"
      },
      "outputs": [],
      "source": [
        "print(val_x.min())\n",
        "print(val_x.max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m-Xvc4e5UoR"
      },
      "source": [
        "The values are between [0, 1], which is good because that means we don't have do any normalization.\n",
        "\n",
        "Now implement a `plot_image()` function which should plot an `x` variable.\n",
        "You can use the `plt.imshow()` function to plot an RGB image.\n",
        "\n",
        "Plot `val_x` and validate that it looks the same as the image file in \"tmp/Datasets/cancer/cancer_data/cancer_data/inputs_test/0/pos_test_000072.png\" in the dataset.\n",
        "\n",
        "*Note: Per PyTorch convention an image has its channels at the 0th dimension (ignoring the batch dimension), but matplotlib expects the channel dimension at the 2nd dimension. You will need to permute the dimensions the plot it correctly*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9CUtuoL5UoR"
      },
      "outputs": [],
      "source": [
        "def plot_image(x):\n",
        "    plt.imshow(x.permute(1, 2, 0))\n",
        "    plt.show()\n",
        "\n",
        "plot_image(val_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG8GdOWW5UoR"
      },
      "source": [
        "Now let's have a look at the cancer labels `y`; print out the shape and dtype of `val_y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGMqrnku5UoR"
      },
      "outputs": [],
      "source": [
        "print(val_y.shape)\n",
        "print(val_y.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAm_r6Ab5UoS"
      },
      "source": [
        "`y` has 1 channel (but it is squeezed so there is no channel dimension).\n",
        "This most likely means that `y` is a grayscale image (which it is).\n",
        "\n",
        "Implement `plot_cancer()` which should plot a `y` variable. You can use the `plt.imshow()` function to plot a grayscale image (pass in the `cmap=\"gray\"` argument to make the image grayscale).\n",
        "\n",
        "Plot `val_y` and validate that it looks like the label image \"tmp/Datasets/cancer/cancer_data/cancer_data/outputs_test/0/pos_test_000072.png\" in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znjvaLMM5UoS"
      },
      "outputs": [],
      "source": [
        "def plot_cancer(y):\n",
        "    plt.imshow(y, cmap=\"gray\")\n",
        "    plt.show()\n",
        "\n",
        "plot_cancer(val_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9lL_gRp5UoS"
      },
      "source": [
        "The cancer is white which means that our cancerous label is 1 and our noncancerous label is 0.\n",
        "Interestingly, the dtype of `y` is a float, but we were expecting cancer labels (i.e. cancerous vs noncancerous).\n",
        "Use `torch.unique()` below to see all the unique values of `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3Pvpqlb5UoS"
      },
      "outputs": [],
      "source": [
        "print(torch.unique(val_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf48qbkV5UoS"
      },
      "source": [
        "While 0 and 1 are in `y`, we have other values inbetween. For classification with cross entropy, we need `y` to be integers only. The original image was only black and white, so these non-integer values are due to the image being resized when it was loaded in. We need to think about the dataset and what we want to accomplish to determine how to treat these values: do we round, truncate, or ceiling?\n",
        "\n",
        "Because this is due to the resizing, these values only occur on the outer edge of the cancer label blobs. For this task, we want to identify regions with cancer, but we don't really need pixel accuracy - we just want the doctors to know to look there for cancer. So, we can truncate these non-one values by calling `y.long()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEMY9kGC5UoT"
      },
      "outputs": [],
      "source": [
        "val_y.long()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0QzZqLE14JT"
      },
      "source": [
        "You should also add a batch dimension to `val_x` and put it on the `device` so we can pass it through the network later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VjS0sla14JT"
      },
      "outputs": [],
      "source": [
        "val_x = val_x.unsqueeze(0).to(device)\n",
        "print(val_x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JWlfiOj5UoT"
      },
      "source": [
        "Now retrieve some baseline accuracy information about the train and validation datasets.\n",
        "- How many images have cancerous cells in the dataset\n",
        "- On average how likely is a pixel going to be cancerous\n",
        "\n",
        "Print this information below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CREFXUHo5UoT"
      },
      "outputs": [],
      "source": [
        "train_num_cancerous = 0\n",
        "train_cancerous_pixels = 0\n",
        "val_num_cancerous = 0\n",
        "val_cancerous_pixels = 0\n",
        "\n",
        "for i in range(len(train_dataset)):\n",
        "    label = train_dataset.label_folder[i][0][0]\n",
        "    train_num_cancerous += 1 if label.long().max() == 1 else 0\n",
        "    train_cancerous_pixels += label.long().sum()\n",
        "\n",
        "for i in range(len(val_dataset)):\n",
        "    label = val_dataset.label_folder[i][0][0]\n",
        "    val_num_cancerous += 1 if label.long().max() == 1 else 0\n",
        "    val_cancerous_pixels += label.long().sum()\n",
        "\n",
        "\n",
        "print(f\"{train_num_cancerous} / {len(train_dataset)} items in the training set include cancerous cells\")\n",
        "print(f\"{train_cancerous_pixels / len(train_dataset) / 512 / 512 * 100:.2f}% of pixels in the training set are labeled as cancerous\")\n",
        "print(f\"{val_num_cancerous} / {len(val_dataset)} items in the validation set include cancerous cells\")\n",
        "print(f\"{val_cancerous_pixels / len(val_dataset) / 512 / 512 * 100:.2f}% of pixels in the validation set are labeled as cancerous\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfWMlPap5UoT"
      },
      "source": [
        "This dataset is very skewed towards non-cancerous data. This makes sense: most cells are non-cancerous, and images that contain cancer will have non-cancerous areas. For our model to be successful, we need to get higher than this baseline accuracy, so our validation accuracy should be over 90%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghxmQAotb9H1"
      },
      "source": [
        "---\n",
        "\n",
        "## Network outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYPQOO5z14JU"
      },
      "source": [
        "\n",
        "Now that we understand our input images and our label images, what should our network's predictions look like?\n",
        "\n",
        "Because we are doing pixelwise classification, we want to output logits (unnormalized probabilities) for each pixel in our input. So the spatial dimension, i.e. HxW, of the network's output will be the same as those of our input.\n",
        "\n",
        "#### Loss\n",
        "If you recall from lab 1, when we implemented cross entropy, our predicted logits had a shape of `(B, K)`, where `K` was the number of classes, and our target classes had a shape of `(B,)`, which was used to index into our predicted logits.\n",
        "This will work similarly for pixelwise cross entropy, the predicted logits $\\hat y$ will have a shape of `(B, K, H, W)`, where `K=2` for our dataset, and the target $y$ will have a shape of `(B, H, W)`.\n",
        "You can use PyTorch's `F.cross_entropy()` function to compute the cross entropy loss.\n",
        "\n",
        "#### Accuracy\n",
        "Implement an accuracy function which compute the pixelwise accuracy between $\\hat y$ and $y$.\n",
        "\n",
        "*Note: Don't forget to use `torch.no_grad()` and since we are just interested in number, return the accuracy as a number, using `.item()`.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3fKUDffAg80"
      },
      "outputs": [],
      "source": [
        "def cancer_detection_accuracy(y_hat, y):\n",
        "    with torch.no_grad():\n",
        "        y = y.to(device)\n",
        "        return (y_hat.argmax(dim=1) == y).float().mean().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IApIBEZiAnYL"
      },
      "source": [
        "Now create a test function for `cancer_detection_accuracy`. Your function should:\n",
        "- Call `cancer_detection_accuracy()` twice, with two different predictions and accuracies.\n",
        "- Be simple, but nontrivial. Don't compare all (non)cancerous predictions with all (non)cancerous targets.\n",
        "\n",
        "Don't forget to add a batch dimension, even if it is just a batch size of 1.\n",
        "\n",
        "*Note: We recommend you create small images since they are easier to visualize and you have different values for each dimension, except for maybe H and W, because `B, C, H, W = 2, 2, 2, 2` can be harder to understand than `B, C, H, W = 1, 2, 3, 3`.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VL5O8zDOAn9e"
      },
      "outputs": [],
      "source": [
        "def test_cancer_detection_accuracy():\n",
        "    y_hat = torch.tensor([[\n",
        "        [\n",
        "            [1, 1, 1],\n",
        "            [0.3, 0.3, 0.3],\n",
        "            [0.6, 0.6, 0.6],\n",
        "        ],\n",
        "        [\n",
        "            [0, 0, 0],\n",
        "            [0.7, 0.7, 0.7],\n",
        "            [0.4, 0.4, 0.4],\n",
        "        ],\n",
        "    ]])\n",
        "\n",
        "    y = torch.tensor([\n",
        "        [\n",
        "            [0,1,0],\n",
        "            [1,0,1],\n",
        "            [0,1,0]\n",
        "        ]\n",
        "    ])\n",
        "    assert math.isclose(cancer_detection_accuracy(y_hat, y), 2/3, rel_tol=1e-6)\n",
        "\n",
        "test_cancer_detection_accuracy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ0e84yN5UoT"
      },
      "source": [
        "---\n",
        "\n",
        "## Dataloaders\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c_Bsbq914JU"
      },
      "source": [
        "\n",
        "Now that we have the datasets, put them into dataloaders for training. Define a batch size (should be pretty small because we'll use a huge network). You should always shuffle your training dataset, but this time it is even more important because the dataset has all negative examples first. If you don't shuffle, your network will learn to predict all pixels as non-cancerous."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmqtqXl5BdOs"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqMjbQX9-ND9"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 2: Training and Validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QD-Pxgh14JV"
      },
      "source": [
        "\n",
        "Now that we understand the data, we'll create training and validation functions BEFORE creating our network.\n",
        "\n",
        "We'll start with validation, since its simpler. Create a validation function that takes in a network and validation dataloader and returns the mean cross-entropy loss (just the value, not the tensor) and pixel-wise accuracy over the dataset. Don't forget to use `torch.no_grad()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJIUcQxE_I7J"
      },
      "outputs": [],
      "source": [
        "def validation(net, val_loader):\n",
        "    loss = 0\n",
        "    accuracy = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_hat = net(x)\n",
        "            loss += F.cross_entropy(y_hat, y.long())\n",
        "            accuracy += cancer_detection_accuracy(y_hat, y)\n",
        "    return loss / len(val_loader), accuracy / len(val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npvXwNkjBHEx"
      },
      "source": [
        "Because this is a very visual task, we also want to see how our model's predictions change over time. We'll use a single image: `val_dataset[172]` and store our model's prediction on it each time we perform validation. At the end, we can print out the images to see how our model improves over time. Create a function that takes in an image tensor of shape `(1, C, H, W)` (batch size of 1) and returns a prediction image of shape `(H, W)`, i.e remove the batch dimension and argmax the channel dimension to get the pixel predictions. Don't forget to use `torch.no_grad()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hOw61JBBIV1"
      },
      "outputs": [],
      "source": [
        "def get_prediction(net, x):\n",
        "    with torch.no_grad():\n",
        "        return net(x).argmax(dim=1).squeeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwGOr8vYFtdR"
      },
      "source": [
        "Create a function(s) to plot your training/validation losses on the same plot and your training/validation accuracies on the same plot.\n",
        "Label your axes and add a legend to make your plots legible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIOlpIwbF5W5"
      },
      "outputs": [],
      "source": [
        "def plot_losses(train_losses, val_losses, train_accs, val_accs, log_interval, val_interval):\n",
        "    import torch\n",
        "\n",
        "    def to_float_list(x):\n",
        "        out = []\n",
        "        for t in x:\n",
        "            if torch.is_tensor(t):\n",
        "                out.append(t.cpu().item())\n",
        "            elif isinstance(t, tuple):\n",
        "                out.append(float(t[0]))\n",
        "            else:\n",
        "                out.append(float(t))\n",
        "        return out\n",
        "\n",
        "    train_losses = to_float_list(train_losses)\n",
        "    val_losses   = to_float_list(val_losses)\n",
        "    train_accs   = to_float_list(train_accs)\n",
        "    val_accs     = to_float_list(val_accs)\n",
        "\n",
        "    train_steps = [i * log_interval for i in range(len(train_losses))]\n",
        "    val_steps = [i * val_interval for i in range(len(val_losses))]\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_steps, train_losses, label=\"Training Loss\")\n",
        "    plt.plot(val_steps, val_losses, label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Steps\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_steps, train_accs, label=\"Training Accuracy\")\n",
        "    plt.plot(val_steps, val_accs, label=\"Validation Accuracy\")\n",
        "    plt.xlabel(\"Steps\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kTjQMct14JV"
      },
      "source": [
        "Create another function to plot 5 image predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUirh3ti14JV"
      },
      "outputs": [],
      "source": [
        "def plot_predictions(val_images):\n",
        "    plt.figure(figsize = (10, 4))\n",
        "    for i in range(len(val_images)):\n",
        "        img = val_images[i].detach().cpu().squeeze().numpy()\n",
        "\n",
        "        plt.subplot(1, len(val_images), i + 1)\n",
        "        plt.imshow(img, cmap=\"gray\")\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h8o9ncjDSv_"
      },
      "source": [
        "---\n",
        "\n",
        "Now create your training function. This should be similar to what we've done in lab 3, but remember that your data and labels are different sizes from lab 3's dataset.\n",
        "\n",
        "Store your training accuracy and losses every `log_interval` steps and validate every `val_interval` steps. Also store 5 predictions of `val_x` dispersed evenly throughout training.\n",
        "Return the statistics you've gathered (losses, accuracies, and val image predictions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d89dRpiADSAC"
      },
      "outputs": [],
      "source": [
        "def train(net, optimizer, train_dataloader, val_dataloader, n_minibatch_steps, log_interval, val_interval, val_im_delay, val_x):\n",
        "    # Train net on the training data, keeping track of loss, accuracy, and predictions on val_x\n",
        "    # Return losses and accuracies on both training and validation datasets, and 5 images\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    val_images = []\n",
        "    checkpoints = [int(round(s)) for s in np.linspace(val_im_delay, n_minibatch_steps - 1, 5)]\n",
        "\n",
        "    step = 0\n",
        "    while step < n_minibatch_steps:\n",
        "        for x, y in train_dataloader:\n",
        "            if step >= n_minibatch_steps:\n",
        "                break\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_hat = net(x)\n",
        "            loss = F.cross_entropy(y_hat, y.long())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if step % log_interval == 0:\n",
        "                train_losses.append(loss.item())\n",
        "                train_accs.append(cancer_detection_accuracy(y_hat, y))\n",
        "\n",
        "            if step % val_interval == 0:\n",
        "                val_loss, val_acc = validation(net, val_dataloader)\n",
        "                val_losses.append(val_loss)\n",
        "                val_accs.append(val_acc)\n",
        "\n",
        "            if step in checkpoints:\n",
        "                val_images.append(get_prediction(net, val_x))\n",
        "            step += 1\n",
        "\n",
        "    return train_losses, train_accs, val_losses, val_accs, val_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFSbxQqs_IQs"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 3: Training Smallest-Possible Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jClr2tPF14JV"
      },
      "source": [
        "\n",
        "While we want to use the UNet on this problem, we should make sure our training and validation runs with a smaller network. Doing this first makes sure that we can focus on bugs in the training and validation loop before focusing on debugging the network itself. And this makes sure that we understand what our inputs and outputs should be.\n",
        "\n",
        "Before reading further, answer the following question:\n",
        "\n",
        "What is the smallest-possible convolutional network that can solve this task?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSHaDax8EnaO"
      },
      "outputs": [],
      "source": [
        "# CREATE SMALLEST NETWORK\n",
        "small_net = nn.Conv2d(3, 2, kernel_size=1).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZGa-ervEvZR"
      },
      "source": [
        "---\n",
        "\n",
        "You should have created a one-layer network with 3 in-channels and 2 out-channels, with a 1x1 convolution. This network obviously won't learn well, but it will be useful for quickly testing and debugging our training and validation functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6QNhv0yEu0u"
      },
      "outputs": [],
      "source": [
        "# Create an optimizer for your network\n",
        "small_optimizer = optim.Adam(small_net.parameters(), lr=0.001)\n",
        "\n",
        "# Train for a few training steps: enough to gather some stats\n",
        "train_losses, train_accs, val_losses, val_accs, val_images = train(\n",
        "    small_net,\n",
        "    small_optimizer,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    16,\n",
        "    1,\n",
        "    3,\n",
        "    0,\n",
        "    val_dataset[172][0].unsqueeze(0).to(device)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z4rZz4T-wGIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dq8ro4LJFXHJ"
      },
      "outputs": [],
      "source": [
        "# Create your loss and accuracy graphs, and print out the 5 validation images:\n",
        "# We do not expect this network to have learned well.\n",
        "\n",
        "plot_losses(train_losses, val_losses, train_accs, val_accs, 1, 3)\n",
        "plot_predictions(val_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZfLz8iJI4XX"
      },
      "outputs": [],
      "source": [
        "# We don't need this network anymore, so we'll delete it to free up GPU space\n",
        "del small_net, small_optimizer, train_losses, train_accs, val_losses, val_accs, val_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nLBjLkM5UoU"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 4: Implementing the UNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoPIQ5qZ5UoU"
      },
      "source": [
        "Use the “Deep Convolution U-Net” from this paper: [(U-Net: Convolutional Networks for Biomedical Image Segmentation)](https://arxiv.org/pdf/1505.04597.pdf)\n",
        "\n",
        "![(Figure 1)](https://lh3.googleusercontent.com/qnHiB3B2KRxC3NjiSDtY08_DgDGTDsHcO6PP53oNRuct-p2QXCR-gyLkDveO850F2tTAhIOPC5Ha06NP9xq1JPsVAHlQ5UXA5V-9zkUrJHGhP_MNHFoRGnjBz1vn1p8P2rMWhlAb6HQ=w2400)\n",
        "<!-- <img src=\"https://lh3.googleusercontent.com/qnHiB3B2KRxC3NjiSDtY08_DgDGTDsHcO6PP53oNRuct-p2QXCR-gyLkDveO850F2tTAhIOPC5Ha06NP9xq1JPsVAHlQ5UXA5V-9zkUrJHGhP_MNHFoRGnjBz1vn1p8P2rMWhlAb6HQ=w2400\" width=\"1600\" height=\"1200\"/> -->\n",
        "\n",
        "You will implement the UNet with some simplifying adjustments.\n",
        "The original UNet does not pad their convolutions, so their output is smaller than their input.\n",
        "Your input and output are the same size so you should pad your convolutions.\n",
        "The only time height and width should change are from \"max pool\" and the \"up-conv\".\n",
        "This means you no longer need to \"crop\" the cross connections, since both sides of the U should have the same spatial dimensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJhqQGTB5UoU"
      },
      "source": [
        "This is a large complex network structure and due to the cross connections, you can't just stick a bunch of layers into `nn.Sequential`.\n",
        "This network does have a clear repeating pattern of blocks that move down (left side of U) and blocks that move up (right side of U).\n",
        "Just like how we use functions to made our code more modular, we'll create two modules to capture that repeating structure to simplify our final network.\n",
        "\n",
        "Implement `DownBlock` (left side of U): The `DownBlock` incorporates 2 sequential convolution layers (with nonlinearities) followed by a downsampling (`nn.MaxPool2d`).\n",
        "It should return the downsampled image and the cross connection output.\n",
        "You should NOT use `copy` or `clone` on the cross connection output to do the \"copy and crop\" operation; just return the same tensor you passed into `MaxPool2D` layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfX_D3uZ5UoY"
      },
      "outputs": [],
      "source": [
        "# You are welcome (and encouraged) to use the built-in batch normalization, nn.BatchNorm2d, and dropout layers, nn.Dropout.\n",
        "\n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        cross_output = self.conv(x)\n",
        "        return self.pool(cross_output), cross_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGJeZ4mN14JW"
      },
      "source": [
        "Implement `UpBlock` (right side of U): The `UpBlock` incorporates an upsampling (i.e. `nn.ConvTranspose2d`) followed by 2 sequential convolution layers (with nonlinearities). Its input should be a downsampled image, `x`, and the cross connection output, `cross_output`, from a `DownBlock`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CQd-5tW14JW"
      },
      "outputs": [],
      "source": [
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels) -> None:\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cross_output):\n",
        "        x = self.up(x)\n",
        "        x = torch.cat((x, cross_output), dim=1)\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWYrQI-k5UoZ"
      },
      "source": [
        "Test below that your `DownBlock` and `UpBlock` work correctly.\n",
        "Create a function `test_updown_block()` which instantiates a `DownBlock` and `UpBlock` and passes in a dummy image `x = torch.zeros((2, 4, 16, 16))`.\n",
        "The downsampled output of your `DownBlock` should be passed through the provided `mid_conv` layer (representing the bottom layer in the UNet) before being passed to the `UpBlock`.\n",
        "You will assert that the the cross-connection output of your `DownBlock` and the output of your `UpBlock` have the shape of `(2, 8, 16, 16)`.\n",
        "Remember to use `torch.no_grad()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-86Ovwb95UoZ"
      },
      "outputs": [],
      "source": [
        "def test_updown_block():\n",
        "    mid_conv = nn.Sequential(\n",
        "        nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    down = DownBlock(4, 8)\n",
        "    up = UpBlock(16, 8)\n",
        "\n",
        "    x = torch.zeros((2, 4, 16, 16))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        down_out, cross_out = down(x)\n",
        "        mid_out = mid_conv(down_out)\n",
        "        up_out = up(mid_out, cross_out)\n",
        "\n",
        "    assert down_out.shape == (2, 8, 8, 8)\n",
        "    assert up_out.shape == (2, 8, 16, 16)\n",
        "\n",
        "test_updown_block()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbGkxZvN14JW"
      },
      "source": [
        "Implement the `UNet` using your `UpBlock` and `DownBlock` modules.\n",
        "You will additionally need to add layers for the bottom of the U and the final output layer.\n",
        "You do not need to use the same number of channels as the UNet paper. It is recommended you use fewer channels for faster training and less memory consumption. `[8, 16, 32, 64]` are a good choice for channel sizes.\n",
        "\n",
        "*Note: If you want to store `UpBlock`s and `DownBlock`s in a list then use the `nn.ModuleList()` which is a module that operates like a list, but exposes the modules to PyTorch.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq22IyKanxo_"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, channels=(8, 16, 32, 64)) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.down_modules = nn.ModuleList()\n",
        "        self.down_modules.append(DownBlock(3, channels[0]))\n",
        "        for i in range(len(channels) - 1):\n",
        "            self.down_modules.append(DownBlock(channels[i], channels[i+1]))\n",
        "\n",
        "        self.mid_module = nn.Sequential(\n",
        "            nn.Conv2d(channels[-1], channels[-1]*2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(channels[-1]*2, channels[-1]*2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.up_modules = nn.ModuleList()\n",
        "        self.up_modules.append(UpBlock(channels[-1]*2, channels[-1]))\n",
        "        for i in range(len(channels) - 1, 0, -1):\n",
        "            self.up_modules.append(UpBlock(channels[i], channels[i-1]))\n",
        "        self.output_module = nn.Conv2d(channels[0], 2, kernel_size=1, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        cross_outputs = []\n",
        "        for i in range(len(self.down_modules)):\n",
        "            x, cross_output = self.down_modules[i](x)\n",
        "            cross_outputs.insert(0, cross_output)\n",
        "        x = self.mid_module(x)\n",
        "        for i in range(len(self.up_modules)):\n",
        "            cross_output = cross_outputs[i]\n",
        "            x = self.up_modules[i](x, cross_output)\n",
        "        x = self.output_module(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kBqEC3i14JW"
      },
      "source": [
        "Test `UNet` by making sure that it takes a batch of input images `x` and returns an appropriately-shaped output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laudR91z5UoZ"
      },
      "outputs": [],
      "source": [
        "def test_UNet_shapes():\n",
        "    unet = UNet()\n",
        "    x = torch.zeros((2, 3, 512, 512))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        y = unet(x)\n",
        "        assert y.shape == (2, 2, 512, 512)\n",
        "\n",
        "test_UNet_shapes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhuXf9ky5UoZ"
      },
      "source": [
        "---\n",
        "\n",
        "# Training and Testing UNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBLhWYut14JX"
      },
      "source": [
        "Train `UNet` below and get a better than baseline accuracy. For Colab/Kaggle users, we recommend training with a small `UNet` first on the CPU and then scaling up the `UNet` and training on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkieTbwlYWPS"
      },
      "outputs": [],
      "source": [
        "# Here are some example hyperparameters: feel free to play around with them for better results\n",
        "n_minibatch_steps = 2000 # number of optimization steps\n",
        "train_log_interval = 10 # how many steps between saving training loss and accuracy\n",
        "val_interval = 200 # how many steps between performing the validation loop\n",
        "val_im_delay = 50 # how many steps before we start storing validation image predictions\n",
        "\n",
        "# small hyperparameters\n",
        "# n_minibatch_steps = 16 # number of optimization steps\n",
        "# train_log_interval = 3 # how many steps between saving training loss and accuracy\n",
        "# val_interval = 5 # how many steps between performing the validation loop\n",
        "# val_im_delay = 0 # how many steps before we start storing validation image predictions\n",
        "\n",
        "# TODO: create UNet and train it, storing statistics from training/validating\n",
        "unet = UNet().to(device)\n",
        "# unet = UNet(channels=(2, 4, 8, 16)).to(device)\n",
        "optimizer = optim.Adam(unet.parameters(), lr=0.001)\n",
        "train_losses, train_accs, val_losses, val_accs, val_images = train(\n",
        "    unet,\n",
        "    optimizer,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    n_minibatch_steps,\n",
        "    train_log_interval,\n",
        "    val_interval,\n",
        "    val_im_delay,\n",
        "    val_dataset[172][0].unsqueeze(0).to(device)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_n_net_params(unet))"
      ],
      "metadata": {
        "id": "BSa7bjK0WgKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnrNqpJD14JX"
      },
      "source": [
        "Plot your training/validation losses and accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTg1jyIsYVZN"
      },
      "outputs": [],
      "source": [
        "plot_losses(train_losses, val_losses, train_accs, val_accs, train_log_interval, val_interval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4s92S2_jQOG"
      },
      "source": [
        "Visualize the 5 predictions of `val_x`. Your images should start to look like the `plot_cancer()` image you made above (signs of cancer in the top left corner)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXfG3wClh8an",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "plot_predictions(val_images)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "cs474",
      "language": "python",
      "name": "cs474"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "nteract": {
      "version": "0.28.0"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}